"""LLM Synthesizer — merge scraped data into a USER.md."""

from __future__ import annotations

import litellm

from llmkit import LLMConfig
from whoami.models import ScrapedData

_SYSTEM_PROMPT = """\
你是 ProfileForge，生成 AI Agent 可直接消费的用户画像（USER.md）。

目标：Agent 拿到后立刻知道「这人是谁、擅长什么、关心什么、该怎么跟他说话」。

## 输出结构

只有两个固定 Section：

1. **Identity** — 姓名/网名、身份、时区/地区、一句话 vibe（从 bio/签名/行为推断）
2. **Interaction Guidelines** — 3-5 条指令，告诉 Agent 该用什么语气、风格、偏好与此人交流

其余 Section 完全由你根据数据自由决定。示例（不限于此）：
- 技术人 → 加 Tech Stack、Projects
- 游戏玩家 → 加 Gaming
- 内容创作者 → 加 Content Creation
- 学生/研究者 → 加 Research
- 什么都有 → 每个方面一个 Section

原则：有什么数据就写什么 Section，没有的不要编造。

## 规则

- 这是写给 Agent 的操作手册，不是写给人看的分析报告
- 语气示例："User prefers concise code-first discussion." "Treat as a peer gamer."
- 只写有把握的事实，禁止"待确认""可能""待探索""需进一步确认"
- 禁止元信息：不写数据来源、置信度、后续建议、生成日期
- 具体 > 泛泛：写"Elden Ring 深度玩家"而不是"喜欢游戏"
- 用短句、关键词、破折号，不写段落
- 过滤敏感信息（邮箱、手机号、身份证号等）
- Interaction Guidelines 是最重要的 Section，必须从数据中推断沟通偏好
- 根据信息量决定长度，最多 30 行
- Markdown 格式，emoji 做 Section 标题前缀
"""


def _build_user_prompt(data_list: list[ScrapedData]) -> str:
    parts: list[str] = []
    for data in data_list:
        section = f"### {data.platform}"
        if data.username:
            section += f" (@{data.username})"
        section += f"\nSource: {data.source_url}\n"
        if data.bio:
            section += f"Bio: {data.bio}\n"
        for item in data.items:
            section += f"- [{item.category}] {item.key}: {item.value}\n"
        parts.append(section)
    return "用户的公开信息：\n---\n" + "\n".join(parts) + "\n---\n\n请生成 USER.md："


async def synthesize(
    data_list: list[ScrapedData],
    *,
    llm: LLMConfig | None = None,
) -> str:
    """Call LLM to synthesize scraped data into USER.md content."""
    if not data_list:
        return "# USER.md\n\n_No data collected._\n"

    if llm is None:
        llm = LLMConfig()

    user_prompt = _build_user_prompt(data_list)
    kwargs = llm.to_litellm_kwargs()
    kwargs.update({
        "messages": [
            {"role": "system", "content": _SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": 0.7,
        "max_tokens": 4096,
    })
    response = await litellm.acompletion(**kwargs)
    content: str = response.choices[0].message.content or ""
    # Strip markdown code fences if the LLM wraps the output
    if content.startswith("```"):
        lines = content.split("\n")
        lines = lines[1:]  # remove opening fence
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        content = "\n".join(lines)
    return content.strip() + "\n\n_Generated by whoami v0.1.0_\n"
